**The *K*-Means Project**

Mohammed Kharma\[0000*−*0001*−*8280*−*3285\]

> Department of Computer Science, Birzeit University, Birzeit, Palestine
> mkharmah@birzeit.edu
>
> **1** **Introduction**
>
> Clustering is an unsupervised machine learning approach that groups
> together comparable data elements based on their feature values \[2\].
> Several clustering methods are available, each with its own set of
> advantages and disadvantages based on the nature of the data and the
> use case. The fundamental structure of the data distribution is
> exploited by clustering algorithms, which also provide criteria for
> categorizing data that have matching features \[2\]. K-Means, also
> known as c-Means, is a popular clustering algorithm and the first
> version was suggested in 1957 by Stuart Lloyd as a method for
> pulse-code modulation, his method was kept inside Bell Labs until he
> published it in 1982 \[3\]. K-Means is an unsupervised learning
> algorithm that works on grouping similar data points together based on
> their feature values. The k-mean process works by constructing
> different partitions/clusters for the targeted dataset based on the
> clustering rules and with no previous knowledge of the dataset. Each
> cluster is made up of comparable data points that are very different
> from the points in the other clusters \[1\]. Such a dissimilarity
> metric depends on the underlying data and the algorithm's goal.
> Clustering is a fundamental problem in machine learning since it is
> crucial to many data-driven applications.
>
> The rest of this report is organized as follows, section 2 presents a
> descrip-tion of each dataset. Section 3 presents algorithm
> description, pseudo-code, and mathematical formulation. Section 4
> presents the experiments and its results. Finally, section 5 provides
> some takeaways from this work.
>
> **2** **Dataset**
>
> In this report, we report the results of using the following datasets:
>
> 1\. **Iris dataset:**\
> In 1936, statistician Ronald Fisher developed the dataset as an
> example of discriminant analysis; since then, it has become a common
> dataset that is used in machine learning. The columns in the dataset
> reflect the following characteristics of flowers:
>
> \(a\) Sepal length\
> (b) Sepal width\
> (c) Petal length
>
> 2 M Kharma
>
> \(d\) Petal width\
> (e) Flower class

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="header">
<th colspan="2">In the</th>
<th colspan="9">following , we provide more</th>
<th>i n s i g h t</th>
<th rowspan="22">2</th>
<th>into</th>
<th>the</th>
<th><blockquote>
<p>dataset .</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="4"># c l a s s e s :</th>
<th colspan="8"><blockquote>
<p>3</p>
</blockquote></th>
<th colspan="3" rowspan="13">3</th>
</tr>
<tr class="header">
<th colspan="2"># data</th>
<th colspan="4">points :</th>
<th colspan="6"><blockquote>
<p>150</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="4">RangeIndex :</th>
<th colspan="2">150</th>
<th colspan="3">entries ,</th>
<th colspan="3"><blockquote>
<p>0 to 149</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="8">Data columns ( t o t a l</th>
<th colspan="4"><blockquote>
<p>5 columns ) :</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>#</th>
<th colspan="3">Column</th>
<th colspan="6">Non−Null Count</th>
<th colspan="2"><blockquote>
<p>Dtype</p>
</blockquote></th>
</tr>
<tr class="header">
<th>−−−</th>
<th colspan="4">−−−−−−</th>
<th colspan="5"><blockquote>
<p>−−−−−−−−−−−−−−</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>−−−−−</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>0</th>
<th colspan="3"><blockquote>
<p>0</p>
</blockquote></th>
<th colspan="6"><blockquote>
<p>150 non−null</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="header">
<th>1</th>
<th colspan="3"><blockquote>
<p>1</p>
</blockquote></th>
<th colspan="6"><blockquote>
<p>150 non−null</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>2</th>
<th colspan="3"><blockquote>
<p>2</p>
</blockquote></th>
<th colspan="6"><blockquote>
<p>150 non−null</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="header">
<th>3</th>
<th colspan="3"><blockquote>
<p>3</p>
</blockquote></th>
<th colspan="6"><blockquote>
<p>150 non−null</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>4</th>
<th colspan="3"><blockquote>
<p>4</p>
</blockquote></th>
<th colspan="6"><blockquote>
<p>150 non−null</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>object</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="3">Dataset</th>
<th colspan="9"><blockquote>
<p>d e s c r i p t i o n :</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="10">0</th>
<th colspan="2"><blockquote>
<p>1</p>
</blockquote></th>
</tr>
<tr class="header">
<th>count</th>
<th colspan="6"><blockquote>
<p>150.000000</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>150.000000</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>150.000000</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>150.000000</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>mean</th>
<th colspan="6">5.843333</th>
<th colspan="3">3.054000</th>
<th colspan="2">3.758667</th>
<th colspan="3"><blockquote>
<p>1.198667</p>
</blockquote></th>
</tr>
<tr class="header">
<th>std</th>
<th colspan="6">0.828066</th>
<th colspan="3">0.433594</th>
<th colspan="2">1.764420</th>
<th colspan="3"><blockquote>
<p>0.763161</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>min</th>
<th colspan="6">4.300000</th>
<th colspan="3">2.000000</th>
<th colspan="2">1.000000</th>
<th colspan="3"><blockquote>
<p>0.100000</p>
</blockquote></th>
</tr>
<tr class="header">
<th>25%</th>
<th colspan="6">5.100000</th>
<th colspan="3">2.800000</th>
<th colspan="2">1.600000</th>
<th colspan="3"><blockquote>
<p>0.300000</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>50%</th>
<th colspan="6">5.800000</th>
<th colspan="3">3.000000</th>
<th colspan="2">4.350000</th>
<th colspan="3"><blockquote>
<p>1.300000</p>
</blockquote></th>
</tr>
<tr class="header">
<th>75%</th>
<th colspan="6">6.400000</th>
<th colspan="3">3.300000</th>
<th colspan="2">5.100000</th>
<th colspan="3"><blockquote>
<p>1.800000</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>max</th>
<th colspan="6">7.900000</th>
<th colspan="3">4.400000</th>
<th colspan="2">6.900000</th>
<th colspan="3"><blockquote>
<p>2.500000</p>
</blockquote></th>
</tr>
</thead>
<tbody>
</tbody>
</table>

> Dataset sample :

+-----------+-----------+-----------+-----------+-----------+-----------+
| 0         | 0         | 1         | 2         | 3         | 4         |
+===========+===========+===========+===========+===========+===========+
|           | 5.1       | 3.5       | 1.4       | 0.2       | > I r i s |
|           |           |           |           |           | > −setosa |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 1         | 4.9       | 3.0       | 1.4       | 0.2       | > I r i s |
|           |           |           |           |           | > −setosa |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 2         | 4.7       | 3.2       | 1.3       | 0.2       | > I r i s |
|           |           |           |           |           | > −setosa |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 3         | 4.6       | 3.1       | 1.5       | 0.2       | > I r i s |
|           |           |           |           |           | > −setosa |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 4         | 5.0       | 3.6       | 1.4       | 0.2       | > I r i s |
|           |           |           |           |           | > −setosa |
+-----------+-----------+-----------+-----------+-----------+-----------+

> .
>
> 2\. **Three separable gaussians:**\
> Three Separable Gaussians is a simulated dataset and contains three
> Gaus-sian distributions that are well separated from one another where
> it has two features, x and y. Also, each data point has a class that
> refer to the cluster it belongs to. The means and standard deviations
> of the three Gaussians are chosen in a way that the clusters are well
> separated and clearly distinct.

+---------+---------+---------+---------+---------+---------+---------+
| In the  | fo      |         | i n s i | into    | the     | >       |
|         | llowing |         | g h t   |         |         | dataset |
|         | , we    |         |         |         |         | > .     |
|         | provide |         |         |         |         |         |
|         | more    |         |         |         |         |         |
+=========+=========+=========+=========+=========+=========+=========+
| \# c l  |         | > 3     |         |         |         |         |
| a s s e |         |         |         |         |         |         |
| s :     |         |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+

<table style="width:100%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="header">
<th colspan="13">The <em>K</em>-Means Project</th>
<th rowspan="20">3</th>
</tr>
<tr class="odd">
<th colspan="2"># data</th>
<th colspan="5">points :</th>
<th colspan="6"><blockquote>
<p>300</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="3">Dataset</th>
<th colspan="10"><blockquote>
<p>information :</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="5">RangeIndex :</th>
<th colspan="2">300</th>
<th colspan="2">entries ,</th>
<th colspan="4"><blockquote>
<p>0 to 299</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="8">Data columns ( t o t a l</th>
<th colspan="5"><blockquote>
<p>3 columns ) :</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>#</th>
<th colspan="4">Column</th>
<th colspan="5">Non−Null Count</th>
<th colspan="3"><blockquote>
<p>Dtype</p>
</blockquote></th>
</tr>
<tr class="header">
<th>−−−</th>
<th colspan="5">−−−−−−</th>
<th colspan="4"><blockquote>
<p>−−−−−−−−−−−−−−</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>−−−−−</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>0</th>
<th colspan="4"><blockquote>
<p>0</p>
</blockquote></th>
<th colspan="5"><blockquote>
<p>300 non−null</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="header">
<th>1</th>
<th colspan="4"><blockquote>
<p>1</p>
</blockquote></th>
<th colspan="5"><blockquote>
<p>300 non−null</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>2</th>
<th colspan="4"><blockquote>
<p>2</p>
</blockquote></th>
<th colspan="5"><blockquote>
<p>300 non−null</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="4">Dataset</th>
<th colspan="9"><blockquote>
<p>d e s c r i p t i o n :</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="8">0</th>
<th colspan="4">1</th>
<th>2</th>
</tr>
<tr class="header">
<th>count</th>
<th colspan="7">300.000000</th>
<th colspan="2">300.000000</th>
<th colspan="3"><blockquote>
<p>300.000000</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>mean</th>
<th colspan="7">0.433244</th>
<th colspan="2"><blockquote>
<p>2.688586</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>1.000000</p>
</blockquote></th>
</tr>
<tr class="header">
<th>std</th>
<th colspan="7">1.618409</th>
<th colspan="2"><blockquote>
<p>1.566672</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>0.817861</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>min</th>
<th colspan="7"><blockquote>
<p>−2.948656</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>−0.765892</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>0.000000</p>
</blockquote></th>
</tr>
<tr class="header">
<th>25%</th>
<th colspan="7"><blockquote>
<p>−1.091289</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>1.162992</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>0.000000</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>50%</th>
<th colspan="7">0.826162</th>
<th colspan="2"><blockquote>
<p>2.909197</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>1.000000</p>
</blockquote></th>
</tr>
<tr class="header">
<th>75%</th>
<th colspan="7">1.697028</th>
<th colspan="2"><blockquote>
<p>4.030363</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>2.000000</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>max</th>
<th colspan="7">3.437618</th>
<th colspan="2"><blockquote>
<p>5.474253</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>2.000000</p>
</blockquote></th>
</tr>
</thead>
<tbody>
</tbody>
</table>

> Dataset sample :

+-----------------+-----------------+-----------------+-----------------+
| 0               | 0               | 1               | > 2             |
+=================+=================+=================+=================+
|                 | 0.428577        | 4.973997        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+
| 1               | 1.619909        | 0.067645        | > 1.0           |
+-----------------+-----------------+-----------------+-----------------+
| 2               | 1.432893        | 4.376792        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+
| 3               | −1.578462       | 3.034458        | > 2.0           |
+-----------------+-----------------+-----------------+-----------------+
| 4               | −1.658629       | 2.267460        | > 2.0           |
+-----------------+-----------------+-----------------+-----------------+

> .
>
> 3\. **Slightly overlapping three gaussians:**
>
> This dataset is comparable to the three separable gaussians dataset in
> terms of features, but with a slight overlap between the three
> Gaussian distribu-tions which makes the clustering of such dataset
> more difficult. The purpose of it is to simulate real cases where the
> boundaries between different clusters are not clearly separable and
> identified.

<table style="width:100%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="header">
<th colspan="2">In the</th>
<th colspan="8">following , we provide more</th>
<th>i n s i g h t</th>
<th rowspan="9">into</th>
<th rowspan="9">the</th>
<th rowspan="9"><blockquote>
<p>dataset .</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="4"># c l a s s e s :</th>
<th colspan="7"><blockquote>
<p>3</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="2"># data</th>
<th colspan="4">points :</th>
<th colspan="5"><blockquote>
<p>300</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="3">Dataset</th>
<th colspan="8"><blockquote>
<p>information :</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="4">RangeIndex :</th>
<th colspan="2">300</th>
<th colspan="2"><blockquote>
<p>entries ,</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>0 to 299</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="7">Data columns ( t o t a l</th>
<th colspan="4"><blockquote>
<p>3 columns ) :</p>
</blockquote></th>
</tr>
<tr class="header">
<th>#</th>
<th colspan="3"><blockquote>
<p>Column</p>
</blockquote></th>
<th colspan="5"><blockquote>
<p>Non−Null Count</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>Dtype</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>−−−</th>
<th colspan="4">−−−−−−</th>
<th colspan="4"><blockquote>
<p>−−−−−−−−−−−−−−</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>−−−−−</p>
</blockquote></th>
</tr>
<tr class="header">
<th>0</th>
<th colspan="3"><blockquote>
<p>0</p>
</blockquote></th>
<th colspan="5">300 non−null</th>
<th colspan="2"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
</thead>
<tbody>
</tbody>
</table>

+-----------+-----------+-----------+-----------+-----------+-----------+
| 4         | > M       |           |           |           | > f l o a |
|           | > Kharma  |           |           |           | > t 6 4   |
+===========+===========+===========+===========+===========+===========+
|           | 1         | 1         | 300       |           |           |
|           |           |           | non−null  |           |           |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | 2         | 2         | 300       |           | > f l o a |
|           |           |           | non−null  |           | > t 6 4   |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > Dataset |           | > d e s c |           | 2         |
|           |           |           | > r i p t |           |           |
|           |           |           | > i o n : |           |           |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | 0         |           |           | 1         |           |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > count   |           | 3         | > 3       | > 3       |
|           |           |           | 00.000000 | 00.000000 | 00.000000 |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > mean    |           | >         | 2.679656  | >         |
|           |           |           |  0.399093 |           |  1.000000 |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > std     |           | >         | 1.721521  | >         |
|           |           |           |  1.752898 |           |  0.817861 |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > min     |           | >         | −1.597670 | >         |
|           |           |           | −3.659532 |           |  0.000000 |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > 25%     |           | >         | 1.227617  | >         |
|           |           |           | −1.006490 |           |  0.000000 |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > 50%     |           | >         | 2.856643  | >         |
|           |           |           |  0.629149 |           |  1.000000 |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > 75%     |           | >         | 4.086663  | >         |
|           |           |           |  1.759564 |           |  2.000000 |
+-----------+-----------+-----------+-----------+-----------+-----------+
|           | > max     |           | >         | 6.059485  | >         |
|           |           |           |  4.128793 |           |  2.000000 |
+-----------+-----------+-----------+-----------+-----------+-----------+

> Dataset sample :

+-----------------+-----------------+-----------------+-----------------+
| 0               | 0               | 1               | > 2             |
+=================+=================+=================+=================+
|                 | > 0.154730      | 5.309102        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+
| 1               | > 1.402230      | −0.347364       | > 1.0           |
+-----------------+-----------------+-----------------+-----------------+
| 2               | > 1.661204      | 4.413295        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+
| 3               | −1.604242       | 3.092746        | > 2.0           |
+-----------------+-----------------+-----------------+-----------------+
| 4               | −1.724491       | 1.942249        | > 2.0           |
+-----------------+-----------------+-----------------+-----------------+

> .
>
> 4\. **Moons**\
> The Moons dataset is widely used for binary classification tasks. It
> consists of two interleaving half-moon shapes, making it a
> non-linearly separable dataset. It contains two features: 1) The
> X-coordinate of the point; 2) The Y-coordinate of the point. And the
> class is the indicating to which half-moon the data point belongs.

<table>
<colgroup>
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th colspan="3">In the</th>
<th colspan="11">following , we provide more</th>
<th>i n s i g h t</th>
<th>into</th>
<th rowspan="16">the</th>
<th rowspan="16"><blockquote>
<p>dataset .</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="4"># c l a s s e s :</th>
<th colspan="12"><blockquote>
<p>2</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="3"># data</th>
<th colspan="4">points :</th>
<th colspan="9"><blockquote>
<p>1000</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="3">Dataset</th>
<th colspan="13"><blockquote>
<p>information :</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="5">RangeIndex :</th>
<th colspan="3">1000</th>
<th colspan="4">entries ,</th>
<th colspan="4"><blockquote>
<p>0 to 999</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="10">Data columns ( t o t a l</th>
<th colspan="6"><blockquote>
<p>3 columns ) :</p>
</blockquote></th>
</tr>
<tr class="header">
<th>#</th>
<th colspan="5">Column</th>
<th colspan="7"><blockquote>
<p>Non−Null Count</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>Dtype</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="2">−−−</th>
<th colspan="4">−−−−−−</th>
<th colspan="7"><blockquote>
<p>−−−−−−−−−−−−−−</p>
</blockquote></th>
<th colspan="3"><blockquote>
<p>−−−−−</p>
</blockquote></th>
</tr>
<tr class="header">
<th>0</th>
<th colspan="2">0</th>
<th colspan="10">1000 non−null</th>
<th colspan="3"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>1</th>
<th colspan="2">1</th>
<th colspan="10">1000 non−null</th>
<th colspan="3"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="header">
<th>2</th>
<th colspan="2">2</th>
<th colspan="10">1000 non−null</th>
<th colspan="3"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="3">Dataset</th>
<th colspan="13"><blockquote>
<p>d e s c r i p t i o n :</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="13">0</th>
<th>1</th>
<th colspan="2">2</th>
</tr>
<tr class="odd">
<th colspan="3">count</th>
<th colspan="6"><blockquote>
<p>1000.000000</p>
</blockquote></th>
<th colspan="5"><blockquote>
<p>1000.000000</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>1000.00000</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="3">mean</th>
<th colspan="8">0.499690</th>
<th colspan="3">0.248688</th>
<th colspan="2">0.50000</th>
</tr>
<tr class="odd">
<th colspan="3">std</th>
<th colspan="8">0.871547</th>
<th colspan="3">0.496743</th>
<th colspan="2">0.50025</th>
</tr>
</thead>
<tbody>
</tbody>
</table>

+-------------+-------------+-------------+-------------+-------------+
| min         | > −1.120606 | −0.604452   | The         | 5           |
|             |             |             | *K*-Means   |             |
|             |             |             | Project     |             |
+=============+=============+=============+=============+=============+
|             |             |             | > 0.00000   |             |
+-------------+-------------+-------------+-------------+-------------+
| 25%         | > −0.043376 | −0.202149   | > 0.00000   |             |
+-------------+-------------+-------------+-------------+-------------+
| 50%         | 0.507788    | 0.242695    | > 0.50000   |             |
+-------------+-------------+-------------+-------------+-------------+
| 75%         | 1.035700    | 0.710452    | > 1.00000   |             |
+-------------+-------------+-------------+-------------+-------------+
| max         | 2.079038    | 1.101272    | > 1.00000   |             |
+-------------+-------------+-------------+-------------+-------------+

> Dataset sample :

+-----------------+-----------------+-----------------+-----------------+
| 0               | 0               | 1               | > 2             |
+=================+=================+=================+=================+
|                 | −1.036507       | 0.392617        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+
| 1               | 1.014714        | 0.177547        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+
| 2               | −0.661602       | 0.705367        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+
| 3               | −0.286087       | 0.967387        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+
| 4               | −0.790062       | 0.615586        | > 0.0           |
+-----------------+-----------------+-----------------+-----------------+

> .
>
> 5\. **Circles**\
> Same as Moons dataset, it's widely used for binary classification
> tasks. It consists of two concentric circles, making it a non-linearly
> separable dataset. The dataset has two features which represent the X
> and Y coordinates, in addition to the class to which circle the data
> point belongs.

<table>
<colgroup>
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th colspan="3">In the</th>
<th colspan="10">following , we provide more</th>
<th>i n s i g h t</th>
<th>into</th>
<th rowspan="21">the</th>
<th rowspan="21"><blockquote>
<p>dataset .</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="4"># c l a s s e s :</th>
<th colspan="11"><blockquote>
<p>2</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="3"># data</th>
<th colspan="4">points :</th>
<th colspan="8"><blockquote>
<p>1000</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="3">Dataset</th>
<th colspan="12"><blockquote>
<p>information :</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="5">RangeIndex :</th>
<th colspan="3">1000</th>
<th colspan="2">entries ,</th>
<th colspan="5"><blockquote>
<p>0 to 999</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="9">Data columns ( t o t a l</th>
<th colspan="6"><blockquote>
<p>3 columns ) :</p>
</blockquote></th>
</tr>
<tr class="header">
<th>#</th>
<th colspan="5">Column</th>
<th colspan="6">Non−Null Count</th>
<th colspan="3"><blockquote>
<p>Dtype</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="2">−−−</th>
<th colspan="4">−−−−−−</th>
<th colspan="6">−−−−−−−−−−−−−−</th>
<th colspan="3"><blockquote>
<p>−−−−−</p>
</blockquote></th>
</tr>
<tr class="header">
<th>0</th>
<th colspan="2">0</th>
<th colspan="8">1000 non−null</th>
<th colspan="4"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="odd">
<th>1</th>
<th colspan="2">1</th>
<th colspan="8">1000 non−null</th>
<th colspan="4"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="header">
<th>2</th>
<th colspan="2">2</th>
<th colspan="8">1000 non−null</th>
<th colspan="4"><blockquote>
<p>f l o a t 6 4</p>
</blockquote></th>
</tr>
<tr class="odd">
<th colspan="3">Dataset</th>
<th colspan="12"><blockquote>
<p>d e s c r i p t i o n :</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="9">0</th>
<th colspan="4">1</th>
<th colspan="2">2</th>
</tr>
<tr class="odd">
<th colspan="3">count</th>
<th colspan="6"><blockquote>
<p>1000.000000</p>
</blockquote></th>
<th colspan="4"><blockquote>
<p>1000.000000</p>
</blockquote></th>
<th colspan="2"><blockquote>
<p>1000.00000</p>
</blockquote></th>
</tr>
<tr class="header">
<th colspan="3">mean</th>
<th colspan="6"><blockquote>
<p>−0.000930</p>
</blockquote></th>
<th colspan="4"><blockquote>
<p>−0.001957</p>
</blockquote></th>
<th colspan="2">0.50000</th>
</tr>
<tr class="odd">
<th colspan="3">std</th>
<th colspan="6">0.560831</th>
<th colspan="4">0.561995</th>
<th colspan="2">0.50025</th>
</tr>
<tr class="header">
<th colspan="3">min</th>
<th colspan="6"><blockquote>
<p>−1.140872</p>
</blockquote></th>
<th colspan="4"><blockquote>
<p>−1.111948</p>
</blockquote></th>
<th colspan="2">0.00000</th>
</tr>
<tr class="odd">
<th colspan="3">25%</th>
<th colspan="6"><blockquote>
<p>−0.439114</p>
</blockquote></th>
<th colspan="4"><blockquote>
<p>−0.444116</p>
</blockquote></th>
<th colspan="2">0.00000</th>
</tr>
<tr class="header">
<th colspan="3">50%</th>
<th colspan="6"><blockquote>
<p>−0.008458</p>
</blockquote></th>
<th colspan="4">0.000451</th>
<th colspan="2">0.50000</th>
</tr>
<tr class="odd">
<th colspan="3">75%</th>
<th colspan="6">0.434248</th>
<th colspan="4">0.436093</th>
<th colspan="2">1.00000</th>
</tr>
<tr class="header">
<th colspan="3">max</th>
<th colspan="6">1.105457</th>
<th colspan="4">1.110823</th>
<th colspan="2">1.00000</th>
</tr>
</thead>
<tbody>
</tbody>
</table>

> Dataset sample :

+-----------------------+-----------------------+-----------------------+
| 0                     | 1                     | > 2                   |
+=======================+=======================+=======================+
+-----------------------+-----------------------+-----------------------+

+-------------+-------------+-------------+-------------+-------------+
| 6           | > M Kharma  |             |             |             |
|             |             |             |             |             |
| **3**       |             |             |             |             |
|             |             |             |             |             |
| **3.1**     |             |             |             |             |
+=============+=============+=============+=============+=============+
|             | 0           | > 1.047437  | > −0.245648 | > 0.0       |
+-------------+-------------+-------------+-------------+-------------+
|             | 1           | > 0.420065  | > 0.168314  | > 1.0       |
+-------------+-------------+-------------+-------------+-------------+
|             | 2           | > 0.223487  | > −0.337189 | > 1.0       |
+-------------+-------------+-------------+-------------+-------------+
|             | 3           | > −0.254356 | > 0.497842  | > 1.0       |
+-------------+-------------+-------------+-------------+-------------+
|             | 4           | > 0.055423  | > −1.014569 | > 0.0       |
+-------------+-------------+-------------+-------------+-------------+
|             | > **Design  |             |             |             |
|             | > and       |             |             |             |
|             | > Imple     |             |             |             |
|             | mentation** |             |             |             |
|             | >           |             |             |             |
|             | **Algorithm |             |             |             |
|             | > ill       |             |             |             |
|             | ustration** |             |             |             |
+-------------+-------------+-------------+-------------+-------------+

> The following illustrates the basic flow of the K-means algorithm:
>
> 1\. Initialize the center of each cluster K by selecting point P (one
> instance from
>
> the dataset) from the dataset randomly as a center for each cluster C
>
> 2\. Loop over the whole dataset instances (points) and assign each
> data point
>
> to the closest cluster center based on calculating the Euclidean
> distance
>
> between each data point and each cluster center. Then assign each
> instance
>
> to the closest cluster center.
>
> 3\. Update the cluster centers K by calculating the mean of all
> instances assigned
>
> in each cluster and update the cluster center of each cluster to be
> the mean
>
> of its data points.
>
> 4\. Repeat steps 2 and 3 until the cluster centers K has reached the
> maximum
>
> number of iterations or algorithm convergence.
>
> 5\. Finally, return the cluster centers and their assigned data points
>
> In the following, the algorithm pseudo-code:
>
> \# I n i t i a l i z e the c l u s t e r centers randomly
>
> clusterCenters *\<*− s e l e c t c instances randomly from the dataset
>
> maxIteration *\<*− some number l i k e 100 i t e r a t i o n
>
> errorLimit *\<*− some small value to control the algorithm convergence
>
> currentLoopIteration *\<*− 0
>
> While maxIteration *\<* currentLoopIteration
>
> \# Assign each data instance to the c l o s e s t *\| \|* error
>
> c l u s t e r\
> rate *\>* errorLimit :

center .

> f or each data point :
>
> Calculate the s i m i l a r i t y measure between the data point and
> each
>
> c l u s t e r center using f or example Euclidean distance function .
>
> Assign the data instances to the nearest c l u s t e r center ' s c l
> u s t e r .
>
> \# Update the c l u s t e r center
>
> f or a l l points per each c l u s t e r :
>
> The *K*-Means Project 7
>
> Calculate the mean of a l l data points assigned to the c l u s t e r
> .
>
> Update the c l u s t e r center of the c l u s t e r to the calculated
> mean .
>
> \# Calculate the error rate / algorithm convergence based on the
> change
>
> in distance between each c l u s t e r point and i t s current center
>
> f or
>
> a l l points per each c l u s t e r :
>
> Calculate the Euclidean distance of a l l data points
>
> assigned to the c l u s t e r .
>
> Get the sum of a l l Euclidean distances f or a l l points (
> currentErrorRate )
>
> and divide i t by the number of instances in the dataset .
>
> i f currentErrorRate*\<*=errorLimit :
>
> break
>
> e l s e
>
> currentLoopIteration++

+--------+--------+--------+--------+--------+--------+--------+--------+
| Return |        | c l u  | center | i t s  | as     | data   | >      |
| each   |        | s t e  | and    |        | signed |        | points |
|        |        | r      |        |        |        |        | > .    |
+========+========+========+========+========+========+========+========+
| *      | > **   |        |        |        |        |        |        |
| *3.2** | Mathem |        |        |        |        |        |        |
|        | atical |        |        |        |        |        |        |
|        | >      |        |        |        |        |        |        |
|        |  equat |        |        |        |        |        |        |
|        | ions** |        |        |        |        |        |        |
+--------+--------+--------+--------+--------+--------+--------+--------+

> To calculate the Euclidian distance between the two instances, assume
> points,
>
> P1 and P2. The Euclidian distance equation is shown in equation 1
> where n refer
>
> to the number of features:

+-----------------------------------------------------------------------+
| > *D*(*P*1*, P*2) =�����(*P*2*i − P*1*i*)2 (1)                        |
| >                                                                     |
| > To calculate the new center in each iteration after the assignment  |
| > step, cal-                                                          |
+=======================================================================+
+-----------------------------------------------------------------------+

> culate the mean of each feature using equation 2 where C is the
> cluster center of
>
> a particular cluster J, X stands for the assigned data points into
> cluster J. and
>
> N is the number of assigned points into cluster J:

+-----------------+-----------------+-----------------+-----------------+
| > *Cj* = 1\     | *Cj*\           | > *Xi*          | \(2\)           |
| > *N*           | �               |                 |                 |
+=================+=================+=================+=================+
+-----------------+-----------------+-----------------+-----------------+

> To calculate the cost function on each iteration, assume we have
> cluster
>
> centers K and data points X. So we need to calculate the distance
> using equation
>
> 1 between each data point in X between i and n with the related
> cluster center
>
> j\. See equation 3:

+-------------+-------------+-------------+-------------+-------------+
| 1*/n ∗*     | 1*\<j\<k*�  | *Xi∈Cj*�    | > *D*(*xi,  | \(3\)       |
|             |             |             | > cj*)      |             |
+=============+=============+=============+=============+=============+
+-------------+-------------+-------------+-------------+-------------+

> 8 M Kharma
>
> **4** **Experiments and results**
>
> **4.1** **Experiments using Iris dataset**
>
> The experiment setup and execution results are as follows:
>
> **--** Dataset: Iris dataset.
>
> **--** Maximum number of iterations: 200 (0-199) iterations.
>
> **--** Initialize features scaling in order to normalize the data
> input to values between zero and one.
>
> **--** Applying dimensionality reduction to 2 classes before running
> k-means in order to help in reducing the computational cost and
> improve the perfor-mance of the algorithm, as well as to identify
> patterns or relationships in the data that may not be easily visible
> in higher dimensions.
>
> **--** Using the number of clusters equal two, the classification
> results with the demonstration of clusters center shown in figure 1.
> The algorithm conver-gence is shown in figure 2.
>
> **--** Using the number of clusters equal to three, the classification
> results with the demonstration of clusters center shown in figure 3.
> The algorithm con-vergence is shown in figure 4. The X-axis refers to
> the number of iterations we use to update and enhance the selection of
> the cluster centers. The Y-axis refers to the calculated cost function
> as per to 3.
>
> **--** Considering that we don't know the distribution of the data and
> we don't have prior knowledge about the actual number of classes in
> the dataset.
>
> Hence, we are dealing with an unlabeled dataset, then we run the
> clustering over a different number of clusters starting from two
> clusters to 9 clusters to test the best data points distribution over
> each cluster center. We stop the iteration when we reach the point
> where the difference between the error rate in the previous iteration
> is less than or equal to 0.02. Figure 5 shows the dataset over the
> best number of found clusters. Figure 6 shows the algorithm iteration
> convergence over a number of iterations to find the best number of
> clusters (The baseline of numbers starts from 2, and the X-axis refers
> to the number of clusters, the Y-axis refers to the cost function).
>
> **4.2** **Experiments using three separable gaussians dataset**
>
> The experiment setup and execution results are as follows:
>
> **--** Dataset: Three separable gaussians dataset.
>
> **--** Maximum number of iterations: 200 (0-199) iterations.
>
> **--** Initialize features scaling in order to normalize the data
> input to values between zero and one.
>
> **--** Using the number of clusters equal two, the classification
> results with the demonstration of clusters center shown in figure 7.
> The algorithm conver-gence is shown in figure 8.
>
> The *K*-Means Project 9
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image1.png){width="3.9375in"
> height="3.15in"}

**Fig. 1.** Iris dataset over 2 clusters .

> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image2.png){width="3.9375in"
> height="2.9541666666666666in"}
>
> **Fig. 2.** Iris dataset - Algorithm iteration convergence - 2
> clusters.
>
> 10 M Kharma
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image3.png){width="3.9375in"
> height="3.15in"}

**Fig. 3.** Iris dataset over 3 clusters .

> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image4.png){width="3.9375in"
> height="2.9541666666666666in"}
>
> **Fig. 4.** Iris dataset - Algorithm iteration convergence - 3
> clusters.
>
> The *K*-Means Project 11
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image5.png){width="3.9375in"
> height="3.15in"}

**Fig. 5.** Iris dataset over the best number of found clusters.

> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image6.png){width="3.9375in"
> height="2.952777777777778in"}
>
> **Fig. 6.** Iris dataset - Algorithm iteration convergence over a
> number of iterations to
>
> find the best number of clusters
>
> 12 M Kharma
>
> **--** Using the number of clusters equal to three, the classification
> results with the demonstration of clusters center shown in figure 9.
> The algorithm con-vergence is shown in figure 10. The X-axis refers to
> the number of iterations we use to update and enhance the selection of
> the cluster centers. The Y-axis refers to the calculated cost function
> as per to 3.
>
> **--** Considering that we don't know the distribution of the data and
> we don't have prior knowledge about the actual number of classes in
> the dataset.
>
> Hence, we are dealing with an unlabeled dataset, then we run the
> clustering over a different number of clusters starting from two
> clusters to 9 clusters to test the best data points distribution over
> each cluster center. We stop the iteration when we reach the point
> where the difference between the error rate in the previous iteration
> is less than or equal to 0.02. Figure 11 shows the dataset over the
> best number of found clusters. Figure 12 shows the algorithm iteration
> convergence over a number of iterations to find the best number of
> clusters (The baseline of numbers starts from 2, and the X-axis refers
> to the number of clusters, the Y-axis refers to the cost function)
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image7.png){width="3.9375in"
> height="3.1486111111111112in"}

**Fig. 7.** Three separable gaussians dataset over 2 clusters .

> The *K*-Means Project 13
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image8.png){width="3.9375in"
> height="2.952777777777778in"}
>
> **Fig. 8.** Three separable gaussians - Algorithm iteration
> convergence - 2 clusters.
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image9.png){width="3.9375in"
> height="3.15in"}

**Fig. 9.** Three separable gaussians dataset over 3 clusters.

> 14 M Kharma
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image10.png){width="3.9375in"
> height="2.952777777777778in"}
>
> **Fig. 10.** Three separable gaussians - Algorithm iteration
> convergence - 3 clusters.
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image11.png){width="3.9375in"
> height="3.15in"}
>
> **Fig. 11.** Three separable gaussians dataset over the best number of
> found clusters.
>
> The *K*-Means Project 15
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image12.png){width="3.9375in"
> height="2.9527766841644794in"}
>
> **Fig. 12.** Three separable gaussians dataset - Algorithm iteration
> convergence over a
>
> number of iterations to find the best number of clusters
>
> 16 M Kharma
>
> **4.3** **Experiments using slightly overlapping three separable**
> **gaussians dataset**
>
> The experiment setup and execution results are as follows:
>
> **--** Dataset: Slightly overlapping three separable gaussians
> dataset.
>
> **--** Maximum number of iterations: 200 (0-199) iterations.
>
> **--** Initialize features scaling in order to normalize the data
> input to values between zero and one.
>
> **--** Using the number of clusters equal two, the classification
> results with the demonstration of clusters center shown in figure 13.
> The algorithm conver- gence is shown in figure 14\
> **--** Using the number of clusters equal to three, the classification
> results with the demonstration of clusters center shown in figure 15.
> The algorithm con- vergence is shown in figure 16. The X-axis refers
> to the number of iterations we use to update and enhance the selection
> of the cluster centers. The Y-axis refers to the calculated cost
> function as per to 3.
>
> **--** Considering that we don't know the distribution of the data and
> we don't have prior knowledge about the actual number of classes in
> the dataset.
>
> Hence, we are dealing with an unlabeled dataset, then we run the
> clustering over a different number of clusters starting from two
> clusters to 9 clusters to test the best data points distribution over
> each cluster center. We stop the iteration when we reach the point
> where the difference between the error rate in the previous iteration
> is less than or equal to 0.02. Figure 17 shows the dataset over the
> best number of found clusters. Figure 18 shows the algorithm iteration
> convergence over a number of iterations to find the best number of
> clusters (The baseline of numbers starts from 2, and the X-axis refers
> to the number of clusters, the Y-axis refers to the cost function)
>
> The *K*-Means Project 17
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image13.png){width="3.9375in"
> height="3.15in"}
>
> **Fig. 13.** Slightly overlapping three separable gaussians dataset
> over 2 clusters.
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image14.png){width="3.9375in"
> height="2.9527766841644794in"}
>
> **Fig. 14.** Slightly overlapping three separable gaussians -
> Algorithm iteration conver-
>
> gence - 2 clusters.
>
> 18 M Kharma
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image15.png){width="3.9375in"
> height="3.15in"}
>
> **Fig. 15.** Slightly overlapping three separable gaussians dataset
> over 3 clusters.
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image16.png){width="3.9375in"
> height="2.9527766841644794in"}
>
> **Fig. 16.** Slightly overlapping three separable gaussians -
> Algorithm iteration conver-
>
> gence - 3 clusters.
>
> The *K*-Means Project 19
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image17.png){width="3.9375in"
> height="3.15in"}
>
> **Fig. 17.** Slightly overlapping three separable gaussians dataset
> over the best number
>
> of found clusters.
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image18.png){width="3.9375in"
> height="2.952777777777778in"}
>
> **Fig. 18.** Slightly overlapping three separable gaussians dataset -
> Algorithm iteration
>
> convergence over a number of iterations to find the best number of
> clusters
>
> 20 M Kharma
>
> **4.4** **Experiments using Moons dataset**
>
> The experiment setup and execution results are as follows:
>
> **--** Dataset: Moons dataset.
>
> **--** Maximum number of iterations: 200 (0-199) iterations.
>
> **--** Initialize features scaling in order to normalize the data
> input to values between zero and one.
>
> **--** Using the number of clusters equal two, the classification
> results with the demonstration of clusters center shown in figure 19.
> The algorithm conver-gence is shown in figure 20.
>
> **--** Using the number of clusters equal to three, the classification
> results with the demonstration of clusters center shown in figure 21.
> The algorithm con-vergence is shown in figure 22. The X-axis refers to
> the number of iterations we use to update and enhance the selection of
> the cluster centers. The Y-axis refers to the calculated cost function
> as per to 3.
>
> **--** Considering that we don't know the distribution of the data and
> we don't have prior knowledge about the actual number of classes in
> the dataset.
>
> Hence, we are dealing with an unlabeled dataset, then we run the
> clustering over a different number of clusters starting from two
> clusters to 9 clusters to test the best data points distribution over
> each cluster center. We stop the iteration when we reach the point
> where the difference between the error rate in the previous iteration
> is less than or equal to 0.02. Figure 23 shows the dataset over the
> best number of found clusters. Figure 24 shows the algorithm iteration
> convergence over a number of iterations to find the best number of
> clusters (The baseline of numbers starts from 2, and the X-axis refers
> to the number of clusters, the Y-axis refers to the cost function).
>
> The *K*-Means Project 21
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image19.png){width="3.9375in"
> height="3.15in"}

**Fig. 19.** Moons dataset over 2 clusters .

> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image20.png){width="3.9375in"
> height="2.9541666666666666in"}
>
> **Fig. 20.** Moons dataset - Algorithm iteration convergence - 2
> clusters.
>
> 22 M Kharma
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image21.png){width="3.9375in"
> height="3.15in"}

**Fig. 21.** Moons dataset over 3 clusters .

> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image22.png){width="3.9375in"
> height="2.9541666666666666in"}
>
> **Fig. 22.** Moons dataset - Algorithm iteration convergence - 3
> clusters.
>
> The *K*-Means Project 23
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image23.png){width="3.9375in"
> height="3.15in"}
>
> **Fig. 23.** Moons dataset over the best number of found clusters.
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image24.png){width="3.9375in"
> height="2.952777777777778in"}
>
> **Fig. 24.** Moons dataset - Algorithm iteration convergence over a
> number of iterations
>
> to find the best number of clusters
>
> 24 M Kharma
>
> **4.5** **Experiments using Circles dataset**
>
> The experiment setup and execution results are as follows:
>
> **--** Dataset: Circles dataset.
>
> **--** Maximum number of iterations: 200 (0-199) iterations.
>
> **--** Initialize features scaling in order to normalize the data
> input to values between zero and one.
>
> **--** Using the number of clusters equal two, the classification
> results with the demonstration of clusters center shown in figure 25.
> The algorithm conver- gence is shown in figure 26\
> **--** Using the number of clusters equal to three, the classification
> results with the demonstration of clusters center shown in figure 27.
> The algorithm con- vergence is shown in figure 28. The X-axis refers
> to the number of iterations we use to update and enhance the selection
> of the cluster centers. The Y-axis refers to the calculated cost
> function as per to 3.
>
> **--** Considering that we don't know the distribution of the data and
> we don't have prior knowledge about the actual number of classes in
> the dataset.
>
> Hence, we are dealing with an unlabeled dataset, then we run the
> clustering over a different number of clusters starting from two
> clusters to 9 clusters to test the best data points distribution over
> each cluster center. We stop the iteration when we reach the point
> where the difference between the error rate in the previous iteration
> is less than or equal to 0.02. Figure 29 shows the dataset over the
> best number of found clusters. Figure 30 shows the algorithm iteration
> convergence over a number of iterations to find the best number of
> clusters (The baseline of numbers starts from 2, and the X-axis refers
> to the number of clusters, the Y-axis refers to the cost function).
>
> The *K*-Means Project 25
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image25.png){width="3.9375in"
> height="3.15in"}

**Fig. 25.** Circles dataset over 2 clusters .

> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image26.png){width="3.9375in"
> height="2.9541666666666666in"}
>
> **Fig. 26.** Circles dataset - Algorithm iteration convergence - 2
> clusters.
>
> 26 M Kharma
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image27.png){width="3.9375in"
> height="3.15in"}

**Fig. 27.** Circles dataset over 3 clusters .

> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image28.png){width="3.9375in"
> height="2.9541666666666666in"}
>
> **Fig. 28.** Circles dataset - Algorithm iteration convergence - 3
> clusters.
>
> The *K*-Means Project 27
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image29.png){width="3.9375in"
> height="3.15in"}
>
> **Fig. 29.** Circles dataset over the best number of found clusters.
>
> ![](vertopal_68e374c853aa4b109480d9de493d1872/media/image30.png){width="3.9375in"
> height="2.952777777777778in"}
>
> **Fig. 30.** Circles dataset - Algorithm iteration convergence over a
> number of iterations
>
> to find the best number of clusters
>
> 28 M Kharma
>
> **5** **Discussion**
>
> Based on the conducted experiments, the following takeaway about the
> K-means:
>
> **--** Clusters centers based algorithm: Where each cluster center
> represents the mean of all data points belonging to that cluster.
>
> **--** Sensitivity for outlier data points: Where the data point
> distribution is con- trolling the cluster center calculation over each
> round of iterations.
>
> **--** The impact of the selected starting cluster centers: The
> selected Centers of the initial clusters have a substantial influence
> on the final clusters generated by the K-Means method, it is critical
> to carefully select at the initialization time the initial centers of
> the cluster that really reflect the nature of data.**--** Produce same
> results under the same conduction: K-means implementation is
> deterministic where when giving the same number of clusters, the same
> dataset, the same number of maximum iterations, and the same Algorithm
> iteration convergence limit (error rate). The produced clusters will
> be the same.
>
> **--** The implementation considers that any particular data point is
> hard clus-tered and it is assigned to one and only one single cluster
> and is exclusively associated with that cluster.
>
> **--** The number of clusters must be determined before running the
> classification iterations: One of the K-Means algorithm's
> disadvantages is that the number of clusters must be set in advance,
> which can be challenging in many real-world situations where the
> nature of data is not well understood.
>
> **References**
>
> 1\. Ahmed, M., Seraj, R., Islam, S.M.S.: The k-means algorithm: A
> comprehensive survey and performance evaluation. Electronics **9**(8),
> 1295 (2020)

<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="header">
<th>2. Jain,</th>
<th>A.K.,</th>
<th>Murty,</th>
<th>M.N.,</th>
<th>Flynn,</th>
<th>P.J.:</th>
<th>Data</th>
<th>clustering:</th>
<th>A</th>
<th>review.</th>
<th><blockquote>
<p>ACM</p>
</blockquote></th>
</tr>
</thead>
<tbody>
</tbody>
</table>

> Comput. Surv. **31**(3), 264--323 (1999).
> https://doi.org/10.1145/331499.331504,
> https://doi.org/10.1145/331499.331504

<table style="width:100%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="header">
<th>3. Lloyd,</th>
<th>S.P.:</th>
<th>Least</th>
<th colspan="2">squares</th>
<th colspan="2">quantization</th>
<th>in</th>
<th>PCM.</th>
<th>IEEE</th>
<th>Trans.</th>
<th>Inf.</th>
<th><blockquote>
<p>The-</p>
</blockquote></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ory</td>
<td><strong>28</strong>(2),</td>
<td colspan="2">129–136</td>
<td colspan="2">(1982).</td>
<td colspan="7"><blockquote>
<p>https://doi.org/10.1109/TIT.1982.1056489,</p>
</blockquote></td>
</tr>
</tbody>
</table>

> https://doi.org/10.1109/TIT.1982.1056489
